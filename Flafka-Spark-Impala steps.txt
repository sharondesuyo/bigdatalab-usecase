STEPS for Flafka:

1) Upload csv file to /home/cloudera

2) Set-up Kafka
	Copy the config files to the /usr/lib/kafka/config folder
	- connect-standalone.properties
	- connect-file-source.properties
	- server.properties

3) Run kafka
	/usr/lib/kafka$> bin/connect-standalone.sh config/connect-standalone.properties config/connect-file-source.properties
	
4) Set-up Flume Agent - save this as flafka.conf

	# Sources, channels, and sinks are defined per
	# agent name, in this case flume1.
	flume1.sources  = kafka-source-1
	flume1.channels = hdfs-channel-1
	flume1.sinks    = hdfs-sink-1
	 
	# For each source, channel, and sink, set
	# standard properties.
	flume1.sources.kafka-source-1.type = org.apache.flume.source.kafka.KafkaSource
	flume1.sources.kafka-source-1.zookeeperConnect = quickstart.cloudera:2181
	flume1.sources.kafka-source-1.topic = flume.txn2
	flume1.sources.kafka-source-1.batchSize = 10000
	flume1.sources.kafka-source-1.channels = hdfs-channel-1
	 
	flume1.channels.hdfs-channel-1.type   = memory
	flume1.sinks.hdfs-sink-1.channel = hdfs-channel-1
	flume1.sinks.hdfs-sink-1.type = hdfs
	flume1.sinks.hdfs-sink-1.hdfs.writeFormat = Text
	flume1.sinks.hdfs-sink-1.hdfs.fileType = DataStream
	flume1.sinks.hdfs-sink-1.hdfs.filePrefix = test-events
	flume1.sinks.hdfs-sink-1.hdfs.useLocalTimeStamp = true
	flume1.sinks.hdfs-sink-1.hdfs.path = /tmp/kafka/%{topic}/%y-%m-%d
	flume1.sinks.hdfs-sink-1.hdfs.rollCount=10000
	flume1.sinks.hdfs-sink-1.hdfs.rollSize=0
	 
	# Other properties are specific to each type of
	# source, channel, or sink. In this case, we
	# specify the capacity of the memory channel.
	flume1.channels.hdfs-channel-1.capacity = 10000
	flume1.channels.hdfs-channel-1.transactionCapacity = 1000

5) Run Flume Agent
	/home/cloudera$> flume-ng agent -f flafka.conf --name flume1 -Dflume.root.logger=INFO,console
   This will save the data to HDFS.
   
6) From Hive, we created a table that contains the entire hdfs file.
	CREATE EXTERNAL TABLE flumetxn_tmp1 (
		json_response STRING
	)
	STORED AS TEXTFILE LOCATION '/tmp/kafka/flume.txn1/18-11-15';
   
7) From Hive, we created a table that contains the payload from the json_response object.
	INSERT OVERWRITE TABLE flumetxn1_proper
	SELECT
		get_json_object(json_response, '$.payload'),
		json_response
	FROM flumetxn1

8) Start spark shell
	> spark-shell --master yarn-client
	
	Scala file to split big file by country list
	++++++++++++++++++++++++++++++++++++++++++++
	val rdd =sc.textFile("hdfs://quickstart/user/cdhassign/data/gtd_92to11.csv").take(100)
	val country_list = rdd.map{ x=> (x.split(",")(8))}.distinct

	import java.io.{BufferedWriter, FileWriter,FileOutputStream}
	import java.io.{IOException, OutputStreamWriter, PrintWriter}

	val country_list_length = country_list.length
	var i=0
	while (i<country_list_length ) {
	val op = rdd.filter(x=>(x.split(",")(8)==country_list(i)))
	val file = country_list(i)+".txt"
	val writer = new BufferedWriter(new OutputStreamWriter(new FileOutputStream(file)))
	for (x <- op ) {
	writer.write(x + "\n")  // however you want to format it
	}
	writer.close()
	i += 1
	}
	
9)
