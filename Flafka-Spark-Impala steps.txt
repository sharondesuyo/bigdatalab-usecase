STEPS for Flafka:

1) Upload csv file to /home/cloudera

2) Set-up Kafka
	Copy the config files to the /usr/lib/kafka/config folder
	- connect-standalone.properties
	- connect-file-source.properties
	- server.properties

3) Run kafka
	/usr/lib/kafka$> bin/connect-standalone.sh config/connect-standalone.properties config/connect-file-source.properties
	
4) Set-up Flume Agent - save this as flafka.conf

	# Sources, channels, and sinks are defined per
	# agent name, in this case flume1.
	flume1.sources  = kafka-source-1
	flume1.channels = hdfs-channel-1
	flume1.sinks    = hdfs-sink-1
	 
	# For each source, channel, and sink, set
	# standard properties.
	flume1.sources.kafka-source-1.type = org.apache.flume.source.kafka.KafkaSource
	flume1.sources.kafka-source-1.zookeeperConnect = quickstart.cloudera:2181
	flume1.sources.kafka-source-1.topic = flume.txn2
	flume1.sources.kafka-source-1.batchSize = 10000
	flume1.sources.kafka-source-1.channels = hdfs-channel-1
	 
	flume1.channels.hdfs-channel-1.type   = memory
	flume1.sinks.hdfs-sink-1.channel = hdfs-channel-1
	flume1.sinks.hdfs-sink-1.type = hdfs
	flume1.sinks.hdfs-sink-1.hdfs.writeFormat = Text
	flume1.sinks.hdfs-sink-1.hdfs.fileType = DataStream
	flume1.sinks.hdfs-sink-1.hdfs.filePrefix = test-events
	flume1.sinks.hdfs-sink-1.hdfs.useLocalTimeStamp = true
	flume1.sinks.hdfs-sink-1.hdfs.path = /tmp/kafka/%{topic}/%y-%m-%d
	flume1.sinks.hdfs-sink-1.hdfs.rollCount=10000
	flume1.sinks.hdfs-sink-1.hdfs.rollSize=0
	 
	# Other properties are specific to each type of
	# source, channel, or sink. In this case, we
	# specify the capacity of the memory channel.
	flume1.channels.hdfs-channel-1.capacity = 10000
	flume1.channels.hdfs-channel-1.transactionCapacity = 1000

5) Run Flume Agent
	/home/cloudera$> flume-ng agent -f flafka.conf --name flume1 -Dflume.root.logger=INFO,console
   This will save the data to HDFS.
   
6) From Hive, we created a table that contains the entire hdfs file.
	CREATE EXTERNAL TABLE flumetxn_tmp1 (
		json_response STRING
	)
	--STORED AS TEXTFILE LOCATION '/tmp/kafka/flume.txn1/18-11-15';
   	STORED AS TEXTFILE LOCATION '/user/cloudera/data';
   
--7) From Hive, we created a table that contains the payload from the json_response object.
--	INSERT OVERWRITE TABLE flumetxn1_proper
--	SELECT
--		get_json_object(json_response, '$.payload'),
--		json_response
--	FROM flumetxn1

8) Start spark shell
	> spark-shell --master yarn-client
	
	Scala file to split big file by country list
	++++++++++++++++++++++++++++++++++++++++++++
	val rdd =sc.textFile("hdfs://quickstart/user/cdhassign/data/gtd_92to11.csv").take(100)
	val country_list = rdd.map{ x=> (x.split(",")(8))}.distinct

	import java.io.{BufferedWriter, FileWriter,FileOutputStream}
	import java.io.{IOException, OutputStreamWriter, PrintWriter}

	val country_list_length = country_list.length
	var i=0
	while (i<country_list_length ) {
	val op = rdd.filter(x=>(x.split(",")(8)==country_list(i)))
	val file = country_list(i)+".txt"
	val writer = new BufferedWriter(new OutputStreamWriter(new FileOutputStream(file)))
	for (x <- op ) {
	writer.write(x + "\n")  // however you want to format it
	}
	writer.close()
	i += 1
	}
	
9)CREATE EXTERNAL TABLE global_terrorist(
eventid BIGINT,
iyear INT,
imonth INT,
iday INT,
approxdate String,
extendeddate STRING,
resolution STRING,
country INT,
country_txt STRING,
region INT,
region_txt STRING,
provstate STRING,
city STRING,
latitude STRING,
longitude STRING,
specificity STRING,
vicinity STRING,
locationplace STRING,
summary STRING,
crit1 STRING,
crit2 STRING,
crit3 STRING,
doubtterr STRING,
alternative STRING,
alternative_txt STRING,
multiple STRING,
success STRING,
suicide STRING,
attacktype1 STRING,
attacktype1_txt STRING,
attacktype2 STRING,
attacktype2_txt STRING,
attacktype3 STRING,
attacktype3_txt STRING,
targtype1 STRING,
targtype1_txt STRING,
targsubtype1 STRING,
targsubtype1_txt STRING,
corp1 STRING,
target1 STRING,
natlty1 STRING,
natlty1_txt STRING,
targtype2 STRING,
targtype2_txt STRING,
targsubtype2 STRING,
targsubtype2_txt STRING,
corp2 STRING,
target2 STRING,
natlty2 STRING,
natlty2_txt STRING,
targtype3 STRING,
targtype3_txt STRING,
targsubtype3 STRING,
targsubtype3_txt STRING,
corp3 STRING,
target3 STRING,
natlty3 STRING,
natlty3_txt STRING,
gname STRING,
gsubname STRING,
gname2 STRING,
gsubname2 STRING,
gname3 STRING,
ingroup STRING,
ingroup2 STRING,
ingroup3 STRING,
gsubname3 STRING,
motive STRING,
guncertain1 STRING,
guncertain2 STRING,
guncertain3 STRING,
nperps STRING,
nperpcap STRING,
claimed STRING,
claimmode STRING,
claimmode_txt STRING,
claim2 STRING,
claimmode2 STRING,
claimmode2_txt STRING,
claim3 STRING,
claimmode3 STRING,
claimmode3_txt STRING,
compclaim STRING,
weaptype1 STRING,
weaptype1_txt STRING,
weapsubtype1 STRING,
weapsubtype1_txt STRING,
weaptype2 STRING,
weaptype2_txt STRING,
weapsubtype2 STRING,
weapsubtype2_txt STRING,
weaptype3 STRING,
weaptype3_txt STRING,
weapsubtype3 STRING,
weapsubtype3_txt STRING,
weaptype4 STRING,
weaptype4_txt STRING,
weapsubtype4 STRING,
weapsubtype4_txt STRING,
weapdetail STRING,
nkill STRING,
nkillus STRING,
nkillter STRING,
nwound STRING,
nwoundus STRING,
nwoundte STRING,
property STRING,
propextent STRING,
propextent_txt STRING,
propvalue STRING,
propcomment STRING,
ishostkid STRING,
nhostkid STRING,
nhostkidus STRING,
nhours STRING,
ndays STRING,
divert STRING,
kidhijcountry STRING,
ransom STRING,
ransomamt STRING,
ransomamtus STRING,
ransompaid STRING,
ransompaidus STRING,
ransomnote STRING,
hostkidoutcome STRING,
hostkidoutcome_txt STRING,
nreleased STRING,
addnotes STRING,
scite1 STRING,
scite2 STRING,
scite3 STRING,
dbsource STRING,
INT_LOG STRING,
INT_IDEO STRING,
INT_MISC STRING,
INT_ANY STRING,
related STRING
)
ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.OpenCSVSerde'
WITH SERDEPROPERTIES 
(
   "separatorChar" = "\,",
   "quoteChar"     = "\""
)
Stored As TEXTFILE
LOCATION '/user/cloudera/data'
TBLPROPERTIES ('skip.header.line.count'='1')

CREATE TABLE global_terrorist_inter AS SELECT * FROM  global_terrorist;

INVALIDATE METADATA;
